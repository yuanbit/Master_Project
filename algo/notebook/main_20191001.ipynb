{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import csv\n",
    "from collections import Counter\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import MeanShift\n",
    "from itertools import chain, combinations\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import Birch\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import re\n",
    "\n",
    "def decode(labels):\n",
    "    encoding = 'utf-8'\n",
    "\n",
    "    # decode from byte to string\n",
    "    labels = [str(x, encoding) for x in labels]\n",
    "    label_decoded = [x.replace('_', ' ') for x in labels]\n",
    "    \n",
    "    return label_decoded\n",
    "\n",
    "def get_labels_idx(keys, labels_string):\n",
    "    # key: category\n",
    "    # value: tuple of name and index, e.g. ('Andy Murray', 0)\n",
    "    labels = {k: [] for k in keys}\n",
    "\n",
    "    for i in range(len(labels_string)):\n",
    "        for k in keys:\n",
    "            if k in labels_string[i]:\n",
    "                name = labels_string[i].replace(k, \"\")\n",
    "                labels[k].append(i)\n",
    "    return labels\n",
    "\n",
    "def get_clusters_dict(labels):\n",
    "    # key: label\n",
    "    # value: indices of images\n",
    "    clusters = {}\n",
    "\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label not in clusters:\n",
    "            # The label is seen for first time, create a new list.\n",
    "            clusters[label] = [idx]\n",
    "        else:\n",
    "            clusters[label].append(idx)\n",
    "            \n",
    "    return clusters\n",
    "\n",
    "# Create label pairs\n",
    "\n",
    "def create_label_pairs(labels):\n",
    "    \n",
    "    label_pairs = {}\n",
    "    \n",
    "    for key, value in labels.items():\n",
    "        label_pairs[key] = list(itertools.combinations(value, 2)) \n",
    "        \n",
    "    label_pairs_concat = []\n",
    "\n",
    "    for key, value in label_pairs.items():\n",
    "        label_pairs_concat += value\n",
    "        \n",
    "    return label_pairs_concat\n",
    "\n",
    "# F-measure\n",
    "\n",
    "def f_measure(true_labels, cluster_labels, algo):\n",
    "    \n",
    "    true_positive = list(set(true_labels).intersection(cluster_labels))\n",
    "    false_positive = list(set(cluster_labels) - set(true_labels))\n",
    "    false_negative = list(set(true_labels) - set(cluster_labels))\n",
    "\n",
    "    TP = len(true_positive)\n",
    "    FP = len(false_positive)\n",
    "    FN = len(false_negative)\n",
    "    \n",
    "    precision = round(TP/(TP+FP), 2)\n",
    "    \n",
    "    recall = round(TP/(TP+FN), 2)\n",
    "    \n",
    "    f_measure = round(2*((precision*recall)/(precision+recall)), 2)\n",
    "    \n",
    "    print(\"{} F-Measure: {}\".format(algo, f_measure))\n",
    "    print(\"{} Precision: {}\".format(algo, precision))\n",
    "    print(\"{} Recall: {}\".format(algo, recall))\n",
    "    print(\"{} Number of False Positives: {}\".format(algo, FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2180, 512)\n"
     ]
    }
   ],
   "source": [
    "# read in embeddings from ArcFace\n",
    "\n",
    "arcface_embeddings = np.load(\"embeddings/data1_arcface/arcface_embeddings.npy\")\n",
    "arcface_raw_labels = np.load(\"embeddings/data1_arcface/arcface_names.npy\")\n",
    "arcface_raw_labels = decode(arcface_raw_labels)\n",
    "arcface_raw_labels = [re.sub(\".png\", \"\", x) for x in arcface_raw_labels]\n",
    "\n",
    "print(arcface_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2180, 128)\n"
     ]
    }
   ],
   "source": [
    "# read in embeddings from dlib\n",
    "\n",
    "dlib_embeddings = np.load(\"embeddings/data1_dlib/data1_embeddings.npy\")\n",
    "dlib_raw_labels = np.load(\"embeddings/data1_dlib/data1_names.npy\")\n",
    "dlib_raw_labels = [re.sub(\".png\", \"\", x) for x in dlib_raw_labels]\n",
    "\n",
    "print(dlib_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2180, 128)\n",
      "Frank_Lickliter_II_golf\n"
     ]
    }
   ],
   "source": [
    "# read in embeddings from Openface\n",
    "\n",
    "data = list(csv.reader(open(\"embeddings/data1_openface/reps.csv\")))\n",
    "label_s = list(csv.reader(open(\"embeddings/data1_openface/labels.csv\")))\n",
    "\n",
    "# data = list(csv.reader(open(\"embeddings/data1_mtcnn_160_openface/reps.csv\")))\n",
    "# label_s = list(csv.reader(open(\"embeddings/data1_mtcnn_160_openface/labels.csv\")))\n",
    "\n",
    "openface_embeddings = np.asarray(data, dtype=float)\n",
    "openface_raw_labels = []\n",
    "\n",
    "for i in range(len(label_s)):\n",
    "    openface_raw_labels.append(label_s[i][1])\n",
    "\n",
    "openface_raw_labels = [re.sub(\"./datasets/data1_aligned/\", \"\", x) for x in openface_raw_labels]\n",
    "#openface_raw_labels = [re.sub(\"./datasets/data1_mtcnnpy_160/\", \"\", x) for x in openface_raw_labels]\n",
    "openface_raw_labels = [re.sub(\"(?=\\/).*$\", \"\", x) for x in openface_raw_labels]\n",
    "\n",
    "print(openface_embeddings.shape)\n",
    "print(openface_raw_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2180, 512)\n"
     ]
    }
   ],
   "source": [
    "# read in embeddings from FaceNet\",\n",
    "\n",
    "facenet_embeddings = np.load(\"embeddings/data1_facenet/embeddings.npy\")\n",
    "label_strings = np.load(\"embeddings/data1_facenet/label_strings.npy\")\n",
    "facenet_raw_labels = decode(label_strings)\n",
    "\n",
    "print(facenet_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Starting clustering and evaluation\n",
    "\n",
    "keys = [\"tennis\", \"basketball\", \"golf\", \"fighter\", \"soccer\"]\n",
    "\n",
    "# Get label/index dictionary\n",
    "facenet_labels = get_labels_idx(keys, facenet_raw_labels)\n",
    "openface_labels = get_labels_idx(keys, openface_raw_labels)\n",
    "dlib_labels = get_labels_idx(keys, dlib_raw_labels)\n",
    "arcface_labels = get_labels_idx(keys, arcface_raw_labels)\n",
    "\n",
    "## Choose method\n",
    "feature_extraction_method = \"arcface\"\n",
    "\n",
    "if feature_extraction_method == \"openface\":\n",
    "\n",
    "    X = openface_embeddings\n",
    "    # Create ground truth pairs for evaulation\n",
    "    true_label_pairs = create_label_pairs(openface_labels)\n",
    "    \n",
    "elif feature_extraction_method == \"facenet\":\n",
    "    \n",
    "    X = facenet_embeddings\n",
    "    # Create ground truth pairs for evaulation\n",
    "    true_label_pairs = create_label_pairs(facenet_labels)\n",
    "\n",
    "elif feature_extraction_method == \"dlib\":\n",
    "    \n",
    "    X = dlib_embeddings\n",
    "    # Create ground truth pairs for evaulation\n",
    "    true_label_pairs = create_label_pairs(dlib_labels)\n",
    "    \n",
    "elif feature_extraction_method == \"arcface\":\n",
    "    \n",
    "    X = arcface_embeddings\n",
    "    # Create ground truth pairs for evaulation\n",
    "    true_label_pairs = create_label_pairs(arcface_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means F-Measure: 0.54\n",
      "K-means Precision: 0.61\n",
      "K-means Recall: 0.48\n",
      "K-means Number of False Positives: 186440\n",
      "--- 1.8585708141326904 seconds ---\n",
      "\n",
      "HAC F-Measure: 0.42\n",
      "HAC Precision: 0.43\n",
      "HAC Recall: 0.42\n",
      "HAC Number of False Positives: 345606\n",
      "--- 1.4945204257965088 seconds ---\n",
      "\n",
      "Spectral F-Measure: 0.47\n",
      "Spectral Precision: 0.54\n",
      "Spectral Recall: 0.42\n",
      "Spectral Number of False Positives: 219153\n",
      "--- 1.4664878845214844 seconds ---\n",
      "\n",
      "GMM F-Measure: 0.51\n",
      "GMM Precision: 0.58\n",
      "GMM Recall: 0.46\n",
      "GMM Number of False Positives: 208091\n",
      "--- 1.607414960861206 seconds ---\n",
      "\n",
      "Birch F-Measure: 0.41\n",
      "Birch Precision: 0.44\n",
      "Birch Recall: 0.39\n",
      "Birch Number of False Positives: 309016\n",
      "--- 1.7515792846679688 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# K-means \n",
    "num_clusters = 5\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "kmeans = KMeans(n_clusters = num_clusters).fit(X)\n",
    "#print(kmeans.labels_)\n",
    "\n",
    "k_means_clusters = get_clusters_dict(kmeans.labels_)\n",
    "\n",
    "# print(labels)\n",
    "# print(\"\\n\")\n",
    "# print(k_means_clusters)\n",
    "\n",
    "kmeans_label_pairs = create_label_pairs(k_means_clusters)\n",
    "\n",
    "#F-measure\n",
    "\n",
    "f_measure(true_label_pairs, kmeans_label_pairs, \"K-means\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print()\n",
    "\n",
    "# Hierarchical Agglomerative Clustering\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=5, distance_threshold=None).fit(X)\n",
    "hac_clusters = get_clusters_dict(clustering.labels_)\n",
    "\n",
    "hac_label_pairs = create_label_pairs(hac_clusters)\n",
    "\n",
    "f_measure(true_label_pairs, hac_label_pairs, \"HAC\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# Spectral Clustering\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "clustering = SpectralClustering(n_clusters=5).fit(X)\n",
    "\n",
    "spectral_cluster = get_clusters_dict(clustering.labels_)\n",
    "\n",
    "spectral_label_pairs = create_label_pairs(spectral_cluster)\n",
    "\n",
    "f_measure(true_label_pairs, spectral_label_pairs, \"Spectral\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print()\n",
    "\n",
    "# Gaussian Mixture EM\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "gmm_labels = GaussianMixture(n_components=5, init_params='kmeans').fit_predict(X)\n",
    "\n",
    "gmm_clusters = get_clusters_dict(gmm_labels)\n",
    "\n",
    "gmm_label_pairs = create_label_pairs(gmm_clusters)\n",
    "\n",
    "f_measure(true_label_pairs, gmm_label_pairs, \"GMM\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print()\n",
    "\n",
    "# Birch\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "brc = Birch(n_clusters=5, compute_labels=True).fit(X) \n",
    "\n",
    "birch_labels = brc.predict(X)\n",
    "\n",
    "birch_clusters = get_clusters_dict(birch_labels)\n",
    "\n",
    "birch_label_pairs = create_label_pairs(birch_clusters)\n",
    "\n",
    "f_measure(true_label_pairs, birch_label_pairs, \"Birch\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find error pairs\n",
    "\n",
    "true_positive = list(set(true_label_pairs).intersection(hac_label_pairs))\n",
    "false_positive = list(set(hac_label_pairs) - set(true_label_pairs))\n",
    "false_negative = list(set(true_label_pairs) - set(hac_label_pairs))\n",
    "\n",
    "print(false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_decoded[27])\n",
    "print(label_decoded[46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DBSCAN\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# clustering = DBSCAN(eps=1, min_samples= 3).fit(X)\n",
    "# DBSCAN_cluster = get_clusters_dict(clustering.labels_)\n",
    "\n",
    "# print(clustering.labels_)\n",
    "# print(\"\\n\")\n",
    "# print(len(DBSCAN_cluster))\n",
    "# print(\"\\n\")\n",
    "# DBSCAN_label_pairs = create_label_pairs(DBSCAN_cluster)\n",
    "\n",
    "# f_measure(true_label_pairs, DBSCAN_label_pairs, \"DBSCAN\")\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# print()\n",
    "\n",
    "# # Affinity Propagation\n",
    "# start_time = time.time()\n",
    "\n",
    "# clustering = AffinityPropagation().fit(X)\n",
    "\n",
    "# ap_clusters = get_clusters_dict(clustering.labels_)\n",
    "\n",
    "# print(len(ap_clusters))\n",
    "\n",
    "# ap_label_pairs = create_label_pairs(ap_clusters)\n",
    "\n",
    "# f_measure(true_label_pairs, ap_label_pairs, \"Affinity Porpagation\")\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# print()\n",
    "\n",
    "# # Mean shift\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# clustering = MeanShift(bandwidth=1).fit(X)\n",
    "\n",
    "# mean_shift_cluster = get_clusters_dict(clustering.labels_)\n",
    "\n",
    "# print(clustering.labels_)\n",
    "# print(\"\\n\")\n",
    "# print(len(mean_shift_cluster))\n",
    "# print(\"\\n\")\n",
    "# mean_shift_label_pairs = create_label_pairs(mean_shift_cluster)\n",
    "\n",
    "# f_measure(true_label_pairs, mean_shift_label_pairs, \"Mean Shift\")\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2)\n",
    "# principalComponents = pca.fit_transform(X)\n",
    "# reduced_centroids = pca.fit_transform(kmeans.cluster_centers_)\n",
    "\n",
    "# colors = [\"#ffe119\", \"#f032e6\", \"#9A6324\", \"#3cb44b\", \"#e6194B\", \"#f58231\", \"#ffe119\", \"#469990\", \"#42d4f4\", \"#4363d8\", \"#911eb4\"]\n",
    "\n",
    "# # plt.scatter(X[:,0], X[:,1], s=5)\n",
    "\n",
    "# for i in kmeans.labels_:\n",
    "#     color = colors[i]\n",
    "#     for feature in principalComponents[kmeans.labels_ == i]:\n",
    "#         plt.scatter(feature[0], feature[1], marker=\"x\", color=color, s=5, linewidths=5)\n",
    "#     plt.scatter(reduced_centroids[i][0], reduced_centroids[i][1], marker=\"o\", color=color, edgecolors='black',  s=30, linewidths=1)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
